---
title: "Analyze CAM data on micro, meso, macro level for part II of basal attributes article"
author: "Paul Sölder, Julius Fenn"
format:
  html:
    toc: true
    toc-depth: 3
    html-math-method: katex
    number-sections: true
---


# global variables



```{r}
#| label: global variables

### Macro level
runLPA = FALSE # takes quite long
```


# Notes

In graph theory and network analysis, the terms "micro," "meso" and "macro" refer to different scales or levels of analysis within a network. Each of these levels focuses on different types of elements and interactions within the network, allowing for a varied understanding of the structure and dynamics of the network as a whole.

# load data files



```{r}
#| label: load files
#| warning: false

# sets the directory of location of this script as the current directory
# setwd(dirname(rstudioapi::getSourceEditorContext()$path))

### load packages
require(pacman)
p_load('tidyverse', 'jsonlite', 'magrittr', 'xlsx',
       'stargazer', 'psych', 'jtools', 'DT', 'ggstatsplot',
       'lavaan',
       'regsem', 'MplusAutomation', 'igraph', 'shiny', 'ggplot2', 'tidyLPA', 'MultilayerExtraction',
       'Matrix', 'igraph', 'foreach', 'doParallel', 'parallel', 'R.matlab',
       'reticulate',
       'RColorBrewer')

# To install MultilayerExtraction (https://github.com/jdwilson4/MultilayerExtraction):
# install.packages("devtools")
# library(devtools, quietly = TRUE)
#
# #install and load MultilayerExtraction
# devtools::install_github('jdwilson4/multilayer_extraction')
#
library(MultilayerExtraction, quietly = TRUE)



# reticulate::py_config()
# reticulate::py_module_available("pycairo")
# reticulate::py_module_available("cairocffi")



if(!reticulate::py_module_available("igraph")){
  py_install("igraph") # pip install python-igraph
}
if(!reticulate::py_module_available("modularitypruning")){
  py_install("modularitypruning") # pip install modularitypruning
}
if(!reticulate::py_module_available("matplotlib")){
  py_install("matplotlib")
}
if(!reticulate::py_module_available("scipy")){
  py_install("scipy")
}
if(!reticulate::py_module_available("leidenalg")){
  py_install("leidenalg")
}
if(!reticulate::py_module_available("os")){
  py_install("os")
}
if(!reticulate::py_module_available("seed")){
  py_install("seed")
}
if(!reticulate::py_module_available("numpy")){
  py_install("numpy")
}
## install Python modules
# py_install("matplotlib")
# py_install("scipy")
# py_install("cairocffi")
# py_install("pycairo") # pip install pycairo



setwd("outputs/01_dataPreperation/final")
### load questionnaire
questionnaire <- readRDS(file = "questionnaire.rds")

CAMfiles <- readRDS(file = "CAMfiles.rds")

CAMdrawn <- readRDS(file = "CAMdrawn.rds")

CAMaggregated <- readRDS(file = "CAMaggregated.rds")

networkIndicators <- readRDS(file = "networkIndicators.rds")

CAMwordlist <- xlsx::read.xlsx2(file = "CAMwordlist.xlsx", sheetIndex = 1)
CAMwordlist$mean_valence <- as.numeric(CAMwordlist$mean_valence)
CAMwordlist$mean_degree <- as.numeric(CAMwordlist$mean_degree)
# CAMwordlist$mean_transitivity <- as.numeric(CAMwordlist$mean_transitivity)


### load functions
# print(getwd())
setwd("../../../functions")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}


setwd("../functions_CAMapp")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}
rm(i)



### summary function
data_summary <- function(data, varname, groupnames){
  require(plyr)
  summary_func <- function(x, col){
    c(mean = mean(x[[col]], na.rm=TRUE),
      se = sd(x[[col]], na.rm=TRUE) / sqrt(length(x[[col]])))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- plyr::rename(data_sum, c("mean" = varname))
  return(data_sum)
}
```
# Describe sample

```{r}
#| label: sample description

table(questionnaire$socio_sex)
round(x = table(questionnaire$socio_sex) / nrow(questionnaire) * 100, digits = 2)

psych::describe(x = questionnaire[, c("socio_age", "total_min_prolific")])
```


# Micro level

* Micro-level analysis focuses on the smallest units of analysis in a network — the individual nodes and the edges between them.


## Distributions (valence, degree, local clustering coefficient)

Check distributions of

* valence
* degree
* local clustering coefficient (transitivity)


```{r}
#| label: distributions micro level

valenceData <- CAMfiles[[1]]
valenceData$value[valenceData$value == 10] <- 0
valenceData$text <-
  str_replace_all(string = valenceData$text,
                  pattern = "  ",
                  replacement = " ")

tmp_names <- sort(unique(valenceData$text))
length(tmp_names)


hist1_names <- tmp_names[1:16]
hist2_names <- tmp_names[17:length(tmp_names)]


# adjust aesthetics of ggplot
ggplot_theme <- theme(
  axis.title.x = element_blank(),
  axis.title.y = element_text(size = 12),
  axis.text.x = element_text(
    size = 10,
    hjust = 0.5,
    vjust = 0.5,
    face = "plain",
    colour = "black"
  ),
  axis.text.y = element_text(
    size = 12,
    face = "plain",
    colour = "black"
  ),
  panel.border = element_blank(),
  axis.line = element_line(colour = "black"),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.background = element_blank()
)

### plot average valence
#> histograms sorted by mean valence
hist1_names_mean <-
  CAMwordlist[order(CAMwordlist$mean_valence, decreasing = TRUE), "Words"][1:16]
hist2_names_mean <-
  CAMwordlist[order(CAMwordlist$mean_valence, decreasing = TRUE), "Words"][17:33]

valenceData %>% filter(text %in% hist1_names_mean) %>%
  ggplot(aes(x = value)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist1_names_mean),
              ncol = 4,
              nrow = 4) +
  ggtitle("Sorted by avg. valence (first)") + ggplot_theme

valenceData %>% filter(text %in% hist2_names_mean) %>%
  ggplot(aes(x = value)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist2_names_mean),
              ncol = 4,
              nrow = 5) +
  ggtitle("Sorted by avg. valence (second)") + ggplot_theme


### create data for mean degree, transitivity
valenceData$degree <- NA
valenceData$transitivity <- NA

for (i in unique(valenceData$participantCAM)) {
  tmp_deg <- igraph::degree(graph = CAMdrawn[[i]], mode = "total")
  tmp_transitivity <-
    igraph::transitivity(graph = as.undirected(CAMdrawn[[i]]),
                         type = "local")

  counter = 1

  for (n in V(CAMdrawn[[i]])$name) {
    valenceData[valenceData$participantCAM %in% i &
                  valenceData$id == n, "degree"] <- tmp_deg[counter]
    valenceData[valenceData$participantCAM %in% i &
                  valenceData$id == n, "transitivity"] <-
      tmp_transitivity[counter]
    counter = counter + 1
  }
}



### plot average degree
#> histograms sorted by mean degree
hist1_names_degree <-
  CAMwordlist[order(CAMwordlist$mean_degree, decreasing = TRUE), "Words"][1:16]
hist2_names_degree <-
  CAMwordlist[order(CAMwordlist$mean_degree, decreasing = TRUE), "Words"][17:33]


valenceData %>% filter(text %in% hist1_names_degree) %>%
  ggplot(aes(x = degree)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist1_names_degree),
              ncol = 4,
              nrow = 4) +
  ggtitle("Sorted by avg. degree (first)") + ggplot_theme


valenceData %>% filter(text %in% hist2_names_degree) %>%
  ggplot(aes(x = degree)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist2_names_degree),
              ncol = 4,
              nrow = 5) +
  ggtitle("Sorted by avg. degree (second)") + ggplot_theme



### plot average transitivity
#> histograms sorted by mean transitivity
#> ! interpret carefully
CAMwordlist$mean_transitivity <- NA

for(i in 1:nrow(CAMwordlist)){
  CAMwordlist$mean_transitivity[i] <- mean(x = valenceData$transitivity[valenceData$text == CAMwordlist$Words[i]], na.rm = TRUE)
}


hist1_names_transitivity <-
  CAMwordlist[order(CAMwordlist$mean_transitivity, decreasing = TRUE), "Words"][1:16]
hist2_names_transitivity <-
  CAMwordlist[order(CAMwordlist$mean_transitivity, decreasing = TRUE), "Words"][17:33]



valenceData %>% filter(text %in% hist1_names_transitivity) %>%
  ggplot(aes(x = transitivity)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist1_names_transitivity),
              ncol = 4,
              nrow = 4) +
  ggtitle("Sorted by avg. local clustering coefficient (first)") + ggplot_theme


valenceData %>% filter(text %in% hist2_names_transitivity) %>%
  ggplot(aes(x = transitivity)) + geom_histogram() +
  facet_wrap( ~ factor(text, levels = hist2_names_transitivity),
              ncol = 4,
              nrow = 5) +
  ggtitle("Sorted by avg. local clustering coefficient (second)") + ggplot_theme
```

## check bimodal pattern of local clustering coefficient

```{r}
#| label: check bimodal distributions

tmp_transitivity_0 <- valenceData$participantCAM[valenceData$text == "Insekten ähnlich" & valenceData$transitivity == 0 & !is.na(valenceData$transitivity)]
tmp_transitivity_1 <- valenceData$participantCAM[valenceData$text == "Insekten ähnlich" & valenceData$transitivity == 1 & !is.na(valenceData$transitivity)]


plot(CAMdrawn[[tmp_transitivity_0[1]]], edge.arrow.size = .7,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size = 10, vertex.label.cex = .9)
plot(CAMdrawn[[tmp_transitivity_1[1]]], edge.arrow.size = .7,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size = 10, vertex.label.cex = .9)
```


## correlation of distributions micro level

```{r}
#| label: correlation of distributions micro level

cor.plot(r = cor(valenceData[, c("value", "degree", "transitivity")], use = "pairwise"))
```


# Meso level

* Meso-level analysis bridges the micro and macro by focusing on groups and communities within the network.



## Leiden algorithm

```{r}
#| label: run Leiden algorithm


modules <- reticulate::py_module_available("leidenalg") && reticulate::py_module_available("igraph")
modules



adjacency_matrix <- CAMaggregated[[1]]

diag(adjacency_matrix) <- 0
graph_object <-
  graph_from_adjacency_matrix(adjacency_matrix, mode = "undirected")


partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.37)

for (i in sort(unique(partition))) {
  cat(
    "\nfor partion",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix)[partition == i],
    "\n"
  )
}



node.cols <- brewer.pal(max(c(3, partition)), "Pastel1")[partition]


### colour aggregated CAM
g = CAMaggregated[[2]]
g2 = simplify(CAMaggregated[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight * 2


for(i in 1:5){
  plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .4, margin = .1,
     vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*5,
     vertex.label.cex = .9,
     edge.weight=2, edge.width=(E(g2)$weight/35),   vertex.color = node.cols)
}
```


```{r}
graph_object <-
  graph_from_adjacency_matrix(adjacency_matrix, mode = "undirected", weighted = TRUE)
# partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.37)
imc <- cluster_infomap(graph_object, e.weights = E(graph_object)$weight, nb.trials = 100)
membership(imc)
communities(imc)
```


### save final parition solution

```{r}
#| label: save solution Leiden algorithm

setwd("outputs/02_analyses_CAMs/Leiden algorithm aggregated CAM")



h = 1
for (i in sort(unique(partition))) {
  if(h == 1){
    dat_out <- data.frame(parition = i, words = rownames(adjacency_matrix)[partition == i])
  }else{
    dat_out <- rbind(dat_out,
                     data.frame(parition = i, words = rownames(adjacency_matrix)[partition == i]))
  }

  h = h + 1
}



dat_out$mean_valence <- NA
dat_out$sd_valence <- NA
for(i in 1:nrow(dat_out)){
  dat_out$mean_valence[i] <- CAMwordlist$mean_valence[CAMwordlist$Words == dat_out$words[i]]
  dat_out$sd_valence[i] <- CAMwordlist$sd_valence[CAMwordlist$Words == dat_out$words[i]]
}

dat_out$words <- str_replace_all(string = dat_out$words, pattern = " {2,6}", replacement = " ")

dat_out$sd_valence <- as.numeric(dat_out$sd_valence)
dat_out$mean_valence <- round(x = dat_out$mean_valence, digits = 2) 


## save as .xlsx file
xlsx::write.xlsx2(x = dat_out, file = "LeidenAlgorithm_solution.xlsx", row.names = FALSE)
## save as R object
saveRDS(dat_out, file = "LeidenAlgorithm_solution.rds")

## add means for partitions

partitions_means <- dat_out %>% 
  group_by(parition) %>%
  summarise(N = n(), mean = mean (mean_valence), SD = mean(sd_valence))

dat_out$sd_valence <- NULL
dat_out$group_mean <- NA

partitions_means$mean <- round(x = partitions_means$mean, digits = 2) 
partitions_means$SD <- round(x = partitions_means$SD, digits = 2) 

for(i in 1:nrow(partitions_means)){
  tmp_boolean <- dat_out$parition == i
  tmp_boolean <- which(tmp_boolean)[length(which(tmp_boolean))]
  dat_out$group_mean[tmp_boolean] <- paste0(partitions_means$mean[i], " (", partitions_means$SD[i], ")")
}


stargazer(dat_out, type = "html", summary = FALSE, rownames = FALSE, out = "tableAppendixD.html")

stargazer(dat_out, type = "latex", summary = FALSE, rownames = FALSE)
```

Compute all means for all possible tuples of the identified partitions:

```{r}
#| label: save possible hypotheses out of solution Leiden algorithm

setwd("outputs/02_analyses_CAMs/Leiden algorithm aggregated CAM")


# Generate all combinations of 2 elements
combinations <- combn(partitions_means$parition, 2, simplify = FALSE)

for(i in 1:length(combinations)){
  
  tmp <- data.frame(partitionA = combinations[[i]][1], 
             partitionB = combinations[[i]][2], 
             mean = mean(partitions_means$mean[combinations[[i]]]), 
             wordsA = paste0(dat_out$words[dat_out$parition == combinations[[i]][1]], collapse = " \\ "), 
             wordsB = paste0(dat_out$words[dat_out$parition == combinations[[i]][2]], collapse = " \\ "))
  
  if(i == 1){
    dat_out_hypotheses <- tmp
  }else{
    dat_out_hypotheses <- rbind(dat_out_hypotheses, tmp)
  }
}


dat_out_hypotheses <- dat_out_hypotheses[order(dat_out_hypotheses$mean),]


## save as .xlsx file
xlsx::write.xlsx2(x = dat_out_hypotheses, file = "LeidenAlgorithm_hypotheses.xlsx", row.names = FALSE)
## save as R object
saveRDS(dat_out_hypotheses, file = "LeidenAlgorithm_hypotheses.rds")


```




### Leiden algorithm on aggregated multi-edge graph with ModularityPruning (Gibson and Mucha, 2022)

Community detection based on the single-layer multi-edge aggregated graph. Partitions are then pruned with ModularityPruning (http://github.com/ragibson/ModularityPruning) to keep only stable and modularity-optimal solutions.

```{python}
#| eval: false

import numpy as np
import igraph as ig
from modularitypruning import prune_to_stable_partitions
from modularitypruning.leiden_utilities import repeated_parallel_leiden_from_gammas
from modularitypruning.champ_utilities import CHAMP_3D
from modularitypruning.parameter_estimation_utilities import domains_to_gamma_omega_estimates
from modularitypruning.plotting import plot_2d_domains_with_estimates
from modularitypruning.plotting import plot_estimates
from modularitypruning.champ_utilities import CHAMP_2D
from modularitypruning.parameter_estimation_utilities import ranges_to_gamma_estimates
from modularitypruning.plotting import plot_estimates
import matplotlib.pyplot as plt
import os
from random import seed
from python_helpers import print_partition  # our local helper scripts


np.set_printoptions(threshold=np.inf)

with open("outputs/01_dataPreperation/final/CAMaggregated_adj_matrix.csv", "r") as f:
    names = f.readline()

names = names.split('" "')
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrix = np.loadtxt(open("outputs/01_dataPreperation/final/CAMaggregated_adj_matrix.csv", "rb"), delimiter=" ", skiprows=1)


# remove self-loops
np.fill_diagonal(adj_matrix, 0)

G = ig.Graph.Adjacency(matrix=adj_matrix, mode="undirected")

gamma_range = (0, 2)
leiden_gammas = np.linspace(*gamma_range, 10 ** 1) # 5   # 100k runs of Leiden algorithm

seed(12345)
partitions = repeated_parallel_leiden_from_gammas(G, leiden_gammas)

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
seed(12345)
stable_parts = prune_to_stable_partitions(G, partitions, *gamma_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
ranges = CHAMP_2D(G, partitions, gamma_0=0.0, gamma_f=2.0)

# append gamma estimate for each dominant partition onto the CHAMP domains
gamma_estimates = ranges_to_gamma_estimates(G, ranges)

# get some infos about our partitions
for stable_part in stable_parts:
    print_partition(stable_part, partitions, names, gamma_estimates)
    print("\n\n")



# plot gamma estimates and domains of optimality
plt.rc('text', usetex=True)
plt.rc('font', family='serif')
plot_estimates(gamma_estimates)
plt.title(r"CHAMP Domains of Optimality and $\gamma$ Estimates", fontsize=14)
plt.xlabel(r"$\gamma$", fontsize=14)
plt.ylabel("Number of communities", fontsize=14)
plt.show()
```



### Leiden algorithm on multilayer graph with ModularityPruning (Gibson and Mucha, 2022)
Community detection based on a multilayer graph with participants as layers. Partitions are then pruned with ModularityPruning (http://github.com/ragibson/ModularityPruning) to keep only stable and modularity-optimal solutions.

```{python}
#| eval: false

import numpy as np
import igraph as ig
from modularitypruning import prune_to_multilayer_stable_partitions
from modularitypruning.leiden_utilities import repeated_parallel_leiden_from_gammas_omegas
from modularitypruning.champ_utilities import CHAMP_3D
from modularitypruning.parameter_estimation_utilities import domains_to_gamma_omega_estimates
from modularitypruning.plotting import plot_2d_domains_with_estimates
import matplotlib.pyplot as plt
from random import seed
from python_helpers import print_partition  # our local helper scripts

import cairocffi
import scipy.io

np.set_printoptions(threshold=np.inf)

# CAMaggregated = scipy.io.loadmat("outputs/01_dataPreperation/final/CAMaggregated_adj_matrices.mat") # !!!
CAMaggregated = scipy.io.loadmat("outputs/01_dataPreperation/final/CAMaggregated_adj_matrices_onlyOnes.mat") # !!!

# import names for labelling
with open("outputs/01_dataPreperation/final/CAMaggregated_adj_matrix.csv", "r") as f:
    names = f.readline()

names = names.split('" "')
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrices = list(CAMaggregated["multigraph_adj_matrices_list"][0,0])

num_layers = len(adj_matrices)
n_per_layer = 33

# nodes   0..32 are layer0
# nodes  33..65 are layer1
# ...

# layer_vec holds the layer membership of each node
# e.g. layer_vec[5] = 2 means that node 5 resides in layer 2 (the third layer)
layer_vec = [i // n_per_layer for i in range(n_per_layer * num_layers)]
interlayer_edges = [(n_per_layer * layer + v, n_per_layer * layer + v + n_per_layer)
                    for layer in range(num_layers - 1)
                    for v in range(n_per_layer)]


# intralayer edges: we need a list of tuples (i.e. edgelist)
# recode the node indices according to the scheme described above (33..65 is layer1 etc.).
# note that this is unweighted for now (could add weights to the igraph object)
intralayer_edges = []
for i, adj_matrix in enumerate(adj_matrices):
    conn_indices = np.where(adj_matrix)
    x_indices, y_indices = conn_indices
    x_indices += i * n_per_layer
    y_indices += i * n_per_layer
    edges = zip(*(x_indices, y_indices))
    intralayer_edges += edges


G_interlayer = ig.Graph(interlayer_edges)
G_intralayer = ig.Graph(intralayer_edges)
# remove the double edges we got from constructing the igraph objects with symmetric matrices
G_intralayer = G_intralayer.simplify()

# run Leiden on a uniform 32x32 grid (1024 samples) of gamma and omega in [0, 2]
gamma_range = (0, 2)
omega_range = (0, 2)
leiden_gammas = np.linspace(*gamma_range, 32)
leiden_omegas = np.linspace(*omega_range, 32)

seed(12345)
parts = repeated_parallel_leiden_from_gammas_omegas(G_intralayer, G_interlayer, layer_vec, gammas=leiden_gammas, omegas=leiden_omegas)


# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
seed(12345)
stable_parts = prune_to_multilayer_stable_partitions(G_intralayer, G_interlayer, layer_vec,
                                                     "multiplex", parts,
                                                     *gamma_range, *omega_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
seed(12345)
domains = CHAMP_3D(G_intralayer, G_interlayer, layer_vec, parts,
                   gamma_0=gamma_range[0], gamma_f=gamma_range[1],
                   omega_0=omega_range[0], omega_f=omega_range[1])



# append resolution parameter estimates for each dominant partition onto the CHAMP domains
seed(12345)
domains_with_estimates = domains_to_gamma_omega_estimates(G_intralayer, G_interlayer, layer_vec,
                                                          domains, model='multiplex')
for partition in stable_parts:
    print_partition(partition, parts, names, domains_with_estimates, type="multilayer")


# plot resolution parameter estimates and domains of optimality
plt.rc('text', usetex=True)
plt.rc('font', family='serif')
plot_2d_domains_with_estimates(domains_with_estimates, xlim=omega_range, ylim=gamma_range)
plt.title(r"CHAMP Domains and ($\omega$, $\gamma$) Estimates", fontsize=16)
plt.xlabel(r"$\omega$", fontsize=20)
plt.ylabel(r"$\gamma$", fontsize=20)
plt.gca().tick_params(axis='both', labelsize=12)
plt.tight_layout()
plt.show()
```


### MultilayerExtraction algorithm (Wilson et al., 2017)
Community detection based on a multilayer graph with participants as layers using the MultilayerExtraction by Wilson et al., 2017 (https://github.com/jdwilson4/MultilayerExtraction).

```{r}
setwd("outputs/02_analyses_CAMs/MultilayerExtraction")

# recode vertex labels to integers
attributes_names <- c(hist1_names, hist2_names)

# there were differences in whitespace between some of the concepts (2 vs. 3 whitespaces) -> fix it
CAMaggregated[[4]][["node1"]][CAMaggregated[[4]][["node1"]] == "Energie   generierend"] <- "Energie  generierend"
CAMaggregated[[4]][["node2"]][CAMaggregated[[4]][["node2"]] == "Energie   generierend"] <- "Energie  generierend"
CAMaggregated[[4]][["node1"]][CAMaggregated[[4]][["node1"]] == "Energie   speichernd"] <- "Energie  speichernd"
CAMaggregated[[4]][["node2"]][CAMaggregated[[4]][["node2"]] == "Energie   speichernd"] <- "Energie  speichernd"

CAMaggregated[[4]][["node1"]] <- match(CAMaggregated[[4]][["node1"]], attributes_names)
CAMaggregated[[4]][["node2"]] <- match(CAMaggregated[[4]][["node2"]], attributes_names)


# recode layers (CAM IDs) to integers
layer_names <- unique(CAMfiles[[1]]$participantCAM)
CAMaggregated[[4]][["layer"]] <- match(CAMaggregated[[4]][["layer"]], layer_names)

# now we're ready for community extraction using the MultiLayer extraction package
set.seed(12345)

# uncomment to run the algorithm (took around 3 hours on our machine)
# start_time <- Sys.time()
# multilayer_extraction_results <- multilayer.extraction(adjacency = CAMaggregated[[4]], seed = 12345, min.score = 0, prop.sample = .10)
# end_time <- Sys.time()
# end_time - start_time

# saveRDS(multilayer_extraction_results, file = "multilayer_extraction_results.Rda")
multilayer_extraction_results <- readRDS(file = "multilayer_extraction_results.Rda")

plot(multilayer_extraction_results, main = "Diagnostic Plot")


# look at the first ... 20 communities
multilayer_extraction_communities <- data.frame(n_communities = seq(1:20))
multilayer_extraction_communities[paste0("community", 1:20)] <- NA

for(i in 1:20) {
  object <- refine(multilayer_extraction_results, k = i, m = length(CAMdrawn), n = nrow(CAMwordlist))

  for(j in 1:i) {
    multilayer_extraction_communities[i,j+1] <- paste(attributes_names[which(object$Vertices[,j] == 1)], collapse = ", ")
  }
}

DT::datatable(multilayer_extraction_communities, options = list(pageLength = 5))
```


# Macro level

* Macro-level analysis looks at the network as a whole, focusing on large-scale patterns and structures.

## plot aggregated CAM

```{r}
#| label: aggregate CAMs

g = CAMaggregated[[2]]
g2 = simplify(CAMaggregated[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight * 2
# E(g2)$weight[E(g2)$weight == 1] <- NA

V(g2)$color[V(g2)$value <= .5 & V(g2)$value >= -.5] <- "yellow"

V(g2)$shape <- NA
V(g2)$shape <- ifelse(test = V(g2)$color == "yellow", yes = "square", no = "circle")



### > plot multiple times because of random layout
for(i in 1:3){
  plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*5,
     vertex.label.cex = .9,
     edge.weight=2, edge.width=(E(g2)$weight/45))
}
```




## hierachical cluster analysis


```{r}
#| label: hierachical cluster analysis


hc_dat <- data.frame(CAM = unique(CAMfiles[[1]]$CAM), participantCAM = unique(CAMfiles[[1]]$participantCAM))

### create data set
## word vars
for(w in unique(CAMfiles[[1]]$text)){
  varName_w <- str_remove_all(string = str_to_title(string = w, locale = "en"), pattern = " |\\W+")

  hc_dat[[paste0("N_", varName_w)]] <- NA
  hc_dat[[paste0("mean_", varName_w)]] <- NA
  hc_dat[[paste0("SD_", varName_w)]] <- NA
}


## get N, mean, sd of single summarized concepts
verbose = FALSE

for(c in unique(CAMfiles[[1]]$CAM)){
  if(verbose){
      cat("considered CAM: ", c, "\n")
  }
  tmp_CAM_nodes <- CAMfiles[[1]][CAMfiles[[1]]$CAM == c, ]
  tmp_CAM_nodes$value <- ifelse(test = tmp_CAM_nodes$value == 10, yes = 0, no = tmp_CAM_nodes$value)

  for(w in unique(CAMfiles[[1]]$text)){
      if(verbose){
    cat("considered concept: ", w, "\n")
      }

    varName_w <- str_remove_all(string = str_to_title(string = w, locale = "en"), pattern = " |\\W+")
          if(verbose){
    cat("   > the freqeuncy, mean, SD are saved with the prefix N_, mean_, SD_ plus
      word without white spaces: ", varName_w, "\n")
          }

    if(sum(tmp_CAM_nodes$text == w) > 0){
      tmp_CAM_nodes_w <- tmp_CAM_nodes[tmp_CAM_nodes$text == w, ]

      ## add N
      hc_dat[hc_dat$CAM == c, paste0("N_", varName_w)] <- nrow(tmp_CAM_nodes_w)
      ## add mean
      hc_dat[hc_dat$CAM == c, paste0("mean_", varName_w)] <- mean(x = tmp_CAM_nodes_w$value)
      ## add SD, only if > 1
      hc_dat[hc_dat$CAM == c, paste0("SD_", varName_w)] <- sd(x = tmp_CAM_nodes_w$value)
    }
  }
        if(verbose){
  cat("\n")
        }
}


### compute clustering
hc_df <- hc_dat[, str_subset(string = colnames(hc_dat), pattern = "mean_")]
hc_df <- hc_df[, colSums(x = !is.na(hc_df)) >= 2] # only considers concepts which where drawn at least 2 times


hc_df_scaled <- scale(hc_df)

dist.eucl <- dist(hc_df_scaled, method = "euclidean")

hc_cluster <- hclust(dist.eucl, method = "ward.D2") # Ward's method
hc_cluster


set_cutOff <- 25


plot(hc_cluster)
abline(h = set_cutOff, col = "red", lty = 2)
rect.hclust(hc_cluster, h=set_cutOff, border = "tomato")

groups <- cutree(hc_cluster, h=set_cutOff)
# groupsOut <- names(table(groups))[table(groups) >= 2]

table(groups)
aggregate(hc_df, by=list(cluster=groups), mean, na.rm = TRUE)
```




## Latent profile analysis


```{r}
#| label: latent profile analysis

if(runLPA){
  valenceDataShort <-
  valenceData %>% pivot_wider(names_from = text,
                              values_from = value,
                              id_cols = CAM) # equal to hc_df

lpa <- valenceDataShort %>% select(-CAM) %>%
 estimate_profiles(1:9, variances = c("equal", "equal", "varying"),
                        covariances = c("zero", "equal", "zero"))
}
```
